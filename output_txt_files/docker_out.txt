voclabs:~/environment/DevOps_Microservices/project-ml-microservice-kubernetes (main) $ ./make_prediction.sh 
Port: 8000
{
  "prediction": [
    20.35373177134412
  ]
}

 => => naming to docker.io/library/project4                                                                                                                0.0s 
REPOSITORY                    TAG       IMAGE ID       CREATED         SIZE                                                                                     
project4                      latest    c20567001ceb   3 seconds ago   1.38GB
thuydocker/project04          v1        8d9b33b1f3f0   3 days ago      1.34GB
gcr.io/k8s-minikube/kicbase   v0.0.40   c6cc01e60919   2 weeks ago     1.19GB
 * Serving Flask app "app" (lazy loading)
 * Environment: production
   WARNING: Do not use the development server in a production environment.
   Use a production WSGI server instead.
 * Debug mode: on
 * Running on http://0.0.0.0:80/ (Press CTRL+C to quit)
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 125-990-647
[2023-07-31 13:43:51,893] INFO in app: JSON payload: 
{'CHAS': {'0': 0}, 'RM': {'0': 6.575}, 'TAX': {'0': 296.0}, 'PTRATIO': {'0': 15.3}, 'B': {'0': 396.9}, 'LSTAT': {'0': 4.98}}
[2023-07-31 13:43:51,906] INFO in app: Inference payload DataFrame: 
   CHAS     RM    TAX  PTRATIO      B  LSTAT
0     0  6.575  296.0     15.3  396.9   4.98
[2023-07-31 13:43:51,917] INFO in app: Scaling Payload: 
   CHAS     RM    TAX  PTRATIO      B  LSTAT
0     0  6.575  296.0     15.3  396.9   4.98
[2023-07-31 13:43:51,921] INFO in app: Output prediction: [20.35373177134412]
172.17.0.1 - - [31/Jul/2023 13:43:51] "POST /predict HTTP/1.1" 200 -
